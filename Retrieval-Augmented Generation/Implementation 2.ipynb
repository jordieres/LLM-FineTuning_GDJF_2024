{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bac29f-07db-44ba-836b-46c5259ab8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementaci√≥n 2 de vector embeddings - B√∫squeda de preguntas similares sint√©ticas generadas por LLM\n",
    "## Se extrae del user_query preguntas similares, que luego se procesan calculando para cada una de las preguntas\n",
    "## la similitud de vectores, luego se eliminan duplicados y se filtran los 3 mejores globales (de los 9 obtenidos)\n",
    "## Con los resultados obtenidos se obtienen los contextos mas relevantes y sus DOIs y se introduen en el\n",
    "## system prompt del modelo con instrucciones de devolver S√ìLO los DOIs relevantes para el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f655f54-bd0c-4d59-90cd-63b830ce5438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d2tb/home/jugodu/envTFG/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-08-07 20:50:10.307859: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-07 20:50:10.327051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-07 20:50:10.345551: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-07 20:50:10.351305: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-07 20:50:10.368580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-07 20:50:11.180412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "CPU times: user 5.8 s, sys: 917 ms, total: 6.72 s\n",
      "Wall time: 6.03 s\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c731fb4f-15ad-47d3-a4e4-04a6289ab3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.645 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "CPU times: user 8.13 s, sys: 3.55 s, total: 11.7 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Carga de los modelos de embeddings y de lenguaje\n",
    "\n",
    "# Cargar y procesar el dataset (columna \"answer\", que contiene en este caso las preguntas extra√≠das de oraciones)\n",
    "df = pd.read_csv('cleaned_dataset2.csv')\n",
    "texts = df['answer'].tolist()\n",
    "\n",
    "# Cargar modelo de embeddings, all-MiniLM-L6-v2\n",
    "modelRAG = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = modelRAG.encode(texts) #Se calculan los embeddings\n",
    "\n",
    "# Cargar LLM\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8694266e-7c98-4f70-b839-6ae16e73cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  7 16:43:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 6000                Off | 00000000:00:10.0 Off |                  Off |\n",
      "| 36%   48C    P8               7W / 260W |   6401MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     55718      C   .../d2tb/home/jugodu/envTFG/bin/python     6398MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef864d-9eb0-4b98-b2f5-ca7287802795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#query_text = \"Where can I find research on positioning systems using physics models?\"\n",
    "#query_text = \"Where can I find research about new navigational location techniques\"\n",
    "#query_text = \"Tell me where can I read about innovative ways insurance is being used in the boating industry\"\n",
    "#query_text = \"Show me sources with information on innovative insurance implementations.\"\n",
    "query_text = \"Are satellite positioning systems used in conjunction with position estimation models?\"\n",
    "\n",
    "\n",
    "# Define the prompt\n",
    "prompt =\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "            You are a helpful assistant.\n",
    "            Your objective is to take the sentence the user inputs and generate similar sentences of related, interesting subjects.\n",
    "            Try to keep the question/demand format. For example, create ONLY ONE sentence similar to the ones in the examples:\n",
    "            user: Where can I find research about new navigational location tecniques?\n",
    "            assistant: Show me sources regarding innovative satellite positioning systems.\n",
    "            OR\n",
    "            assistant: What are the comprehensive studies that explore the relationship between physics and the broader environment?\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "            {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# Format the prompt with RAG_string\n",
    "formatted_prompt = prompt.format(query_text)\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "## Inferencia para buscar dos preguntas de similar contexto \n",
    "## Pregunta extra 1\n",
    "model_answer1= model.generate(**inputs, temperature=0.7, max_new_tokens = 128, use_cache = True)\n",
    "output1=tokenizer.batch_decode(model_answer1)\n",
    "\n",
    "## Pregunta extra 2\n",
    "model_answer2= model.generate(**inputs, temperature=0.7, max_new_tokens = 128, use_cache = True)\n",
    "output2=tokenizer.batch_decode(model_answer2)\n",
    "\n",
    "## Funci√≥n para extraer SOLO el output del modelo.\n",
    "def output_cleaner(output):\n",
    "    # Convert list to string\n",
    "    output_str = output[0]\n",
    "    # Find the index of \"user\\n\\n\"\n",
    "    user_index = output_str.find(\"assistant<|end_header_id|>\\n\\n            \")\n",
    "    # Slice the string to remove everything up to \"user\\n\\n\"\n",
    "    processed_output = output_str[user_index + len(\"assistant<|end_header_id|>\\n\\n            \"):]\n",
    "    # Remove the last character eot\n",
    "    if processed_output.endswith(\"<|eot_id|>\"):\n",
    "        processed_output = processed_output[:-10]\n",
    "    return processed_output\n",
    "\n",
    "\n",
    "# Similarity Search\n",
    "query_text = query_text #Como recordatorio de que est√° declarada antes\n",
    "query_text_2 = output_cleaner(output1)\n",
    "query_text_3 = output_cleaner(output2)\n",
    "\n",
    "query_embedding1 = modelRAG.encode([query_text])\n",
    "query_embedding2 = modelRAG.encode([query_text_2])\n",
    "query_embedding3 = modelRAG.encode([query_text_3])\n",
    "\n",
    "similarity_scores1 = cosine_similarity(query_embedding1, embeddings).flatten()\n",
    "similarity_scores2 = cosine_similarity(query_embedding2, embeddings).flatten()\n",
    "similarity_scores3 = cosine_similarity(query_embedding3, embeddings).flatten()\n",
    "\n",
    "# Get the indices of the top 3 most similar texts\n",
    "top_3_indices1 = np.argsort(similarity_scores1)[-3:][::-1]\n",
    "top_3_indices2 = np.argsort(similarity_scores2)[-3:][::-1]\n",
    "top_3_indices3 = np.argsort(similarity_scores3)[-3:][::-1]\n",
    "\n",
    "sim_scores_array1 = []\n",
    "sim_scores_array2 = []\n",
    "sim_scores_array3 = []\n",
    "\n",
    "for i in top_3_indices1:\n",
    "    score = similarity_scores1[i]\n",
    "    sim_scores_array1.append(score)\n",
    "\n",
    "for i in top_3_indices2:\n",
    "    score = similarity_scores2[i]\n",
    "    sim_scores_array2.append(score)\n",
    "\n",
    "for i in top_3_indices3:\n",
    "    score = similarity_scores3[i]\n",
    "    sim_scores_array3.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1d185-8f64-4600-b1b1-47cf9f0b709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data as space-separated strings\n",
    "array1 = top_3_indices1\n",
    "list1 = sim_scores_array1\n",
    "array2 = top_3_indices2\n",
    "list2 = sim_scores_array2\n",
    "array3 = top_3_indices3\n",
    "list3 = sim_scores_array3\n",
    "\n",
    "# Combine each array with its corresponding list\n",
    "combined1 = list(zip(array1, list1))\n",
    "combined2 = list(zip(array2, list2))\n",
    "combined3 = list(zip(array3, list3))\n",
    "\n",
    "# Sort based on the first element of each tuple (array value)\n",
    "combined1_sorted = sorted(combined1, key=lambda x: x[0])\n",
    "combined2_sorted = sorted(combined2, key=lambda x: x[0])\n",
    "combined3_sorted = sorted(combined3, key=lambda x: x[0])\n",
    "\n",
    "# Combine all groups into a single list\n",
    "combined_all = combined1 + combined2 + combined3\n",
    "\n",
    "# Sort based on the second element of each tuple (list value), in descending order\n",
    "combined_all_sorted = sorted(combined_all, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create a dictionary to remove duplicates and keep the first occurrence (highest value due to sorting)\n",
    "unique_dict = {}\n",
    "for array_val, list_val in combined_all_sorted:\n",
    "    if array_val not in unique_dict.values():\n",
    "        unique_dict[list_val] = array_val\n",
    "\n",
    "# Unpack the dictionary back into arrays and lists\n",
    "sorted_arrays = np.array(list(unique_dict.values()))\n",
    "sorted_lists = list(unique_dict.keys())\n",
    "\n",
    "# Reduce to the top 3 values\n",
    "sorted_arrays_top3 = sorted_arrays[:3]\n",
    "sorted_lists_top3 = sorted_lists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2079598c-8067-4b33-afe9-01dd28d2c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "            You are a research assistant model.\n",
      "            Your objective is to retrieve Digital Object Identifiers (DOIs) of research papers that relate to the user's query.\n",
      "            Read the context provided and decide if it's relevant to what the user is asking. If it is, provide the DOIs for all the contexts that are relevant.\n",
      "            If you are unsure if the context is relevant for given question, DO NOT make stuff up and instead answer truthfully.\n",
      "            If there are DOIs, reply ONLY with \"A scientific article regarding those subjects can be found with the DOI\" (use the plural if needed), followed by the DOIs that are more relevant (if at all), don't explain your reasoning.\n",
      "            If there are no DOIs, DON'T MAKE THEM UP, explain you couldn't find the answer in your data sources.\n",
      "            If all similarity scores are below 0.45, tell the user you couldn't find research in your training dataset on that topic.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "            \n",
      "Context 1: I'm interested in satellite-based AIS and its capabilities for collecting data globally in real-time; what sources should I look at?\n",
      "DOIs: 10.1109/ants.2016.7947788\n",
      "Similarity Score: 0.5279\n",
      "\n",
      "\n",
      "Context 2: I need references for understanding the use of satellite-based AIS in global data collection.\n",
      "DOIs: 10.1109/ants.2016.7947788\n",
      "Similarity Score: 0.4845\n",
      "\n",
      "\n",
      "Context 3: What are some notable studies on localization and ranging techniques?\n",
      "DOIs: 10.1109/mass.2014.36\n",
      "Similarity Score: 0.4724\n",
      "\n",
      "\n",
      "            Are satellite positioning systems used in conjunction with position estimation models?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A scientific article regarding those subjects can be found with the DOI 10.1109/ants.2016.7947788<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "RAG_string = \"\"\n",
    "for idx, index in enumerate(sorted_arrays_top3):\n",
    "    text = df.loc[sorted_arrays_top3[idx], 'answer']\n",
    "    respuesta_DOIs = df.loc[sorted_arrays_top3[idx], 'DOIs']\n",
    "    respuesta_Titulo = df.loc[sorted_arrays_top3[idx], 'Title']\n",
    "    similarity_score = sorted_lists_top3[idx]\n",
    "    \n",
    "    # Use variables in a multi-line f-string\n",
    "    RAG_string += f\"\"\"\n",
    "Context {idx + 1}: {text}\n",
    "DOIs: {respuesta_DOIs}\n",
    "Similarity Score: {similarity_score:.4f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# He quitado Title: {respuesta_Titulo} despues del output de DOIs: {respuesta_DOIs} \n",
    "\n",
    "# Define the prompt\n",
    "prompt =\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "            You are a research assistant model.\n",
    "            Your objective is to retrieve Digital Object Identifiers (DOIs) of research papers that relate to the user's query.\n",
    "            Read the context provided and decide if it's relevant to what the user is asking. If it is, provide the DOIs for all the contexts that are relevant.\n",
    "            If you are unsure if the context is relevant for given question, DO NOT make stuff up and instead answer truthfully.\n",
    "            If there are DOIs, reply ONLY with \"A scientific article regarding those subjects can be found with the DOI\" (use the plural if needed), followed by the DOIs that are more relevant (if at all), don't explain your reasoning.\n",
    "            If there are no DOIs, DON'T MAKE THEM UP, explain you couldn't find the answer in your data sources.\n",
    "            If all similarity scores are below 0.45, tell the user you couldn't find research in your training dataset on that topic.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "            {}\n",
    "            {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# Format the prompt with RAG_string\n",
    "formatted_prompt = prompt.format(RAG_string, query_text)\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate text\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f77624-aeab-4087-a88b-8eb7db982aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
