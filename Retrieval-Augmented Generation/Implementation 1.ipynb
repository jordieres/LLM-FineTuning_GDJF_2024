{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f2c44-3041-4553-81cb-5fc6a1ad0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementaci√≥n original de vector embeddings - B√∫squeda de preguntas similares y deicisi√≥n con LLM\n",
    "## Se calculan las 3 preguntas en el dataset mas similares a la pregunta del usuario\n",
    "## Con los resultados obtenidos se obtienen los contextos mas relevantes y sus DOIs y se introduen en el\n",
    "## system prompt del modelo con instrucciones de devolver S√ìLO los DOIs relevantes para el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f655f54-bd0c-4d59-90cd-63b830ce5438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 22:26:18.337052: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-02 22:26:18.361632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-02 22:26:18.384022: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-02 22:26:18.390810: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-02 22:26:18.411570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-02 22:26:19.333177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a9fce2-d67c-4580-b90f-366cf8285081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 s, sys: 363 ms, total: 1.72 s\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cargar y procesar el dataset (columna \"answer\", que contiene en este caso las preguntas extra√≠das de oraciones)\n",
    "df2 = pd.read_csv('dataset.csv')\n",
    "texts = df2['answer'].tolist()\n",
    "\n",
    "# Generar Embeddings con el modelo all-MiniLM-L6-v2\n",
    "modelRAG2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = modelRAG2.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c731fb4f-15ad-47d3-a4e4-04a6289ab3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.645 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "CPU times: user 6.82 s, sys: 2.08 s, total: 8.9 s\n",
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8694266e-7c98-4f70-b839-6ae16e73cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 08:57:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 6000                Off | 00000000:00:10.0 Off |                  Off |\n",
      "| 36%   50C    P2              60W / 260W |   6401MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     65586      C   .../d2tb/home/jugodu/envTFG/bin/python     6398MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ef864d-9eb0-4b98-b2f5-ca7287802795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "            You are a research assistant model.\n",
      "            Your objective is to retrieve Digital Object Identifiers (DOIs) of research papers that relate to the user's query.\n",
      "            Read the context provided and decide if it's relevant to what the user is asking. If it is, provide the DOIs for all the contexts that are relevant.\n",
      "            If you are unsure if the context is relevant for given question, try to not make stuff up and instead answer truthfully.\n",
      "            If there a DOIs, reply ONLY with \"A scientific article regarding those subjects can be found with the DOI\" (use the plural if needed), followed by the DOIs that are more relevant (if at all), don't explain your reasoning.\n",
      "            If there are no DOIs, DON'T MAKE THEM UP, explain you couldn't find the answer in your data sources.\n",
      "            If all similarity scores are below 0.45, tell the user you couldn't find research on that topic.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "            \n",
      "Context 1: What are the latest studies on maritime cyber security and its technological aspects?\n",
      "DOIs: A scientific article regarding those subjects can be found with the DOI 10.1080/23738871.2018.1513053\n",
      "Similarity Score: 0.6198\n",
      "\n",
      "\n",
      "Context 2: What are some authoritative sources discussing the impact of digital twins on the maritime sector, particularly in shipping?\n",
      "DOIs: A scientific article regarding those subjects can be found with the DOI 10.3390/jmse11051021\n",
      "Similarity Score: 0.6018\n",
      "\n",
      "\n",
      "Context 3: I'm looking for studies that analyze automation levels in the maritime sector over the years.\n",
      "DOIs: A scientific article regarding those subjects can be found with the DOI 10.1007/978-3-319-68496-3_15\n",
      "Similarity Score: 0.5911\n",
      "\n",
      "\n",
      "            Question: Show me sources on innovative digitalization methods implemented in the maritime industry<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A scientific article regarding those subjects can be found with the DOIs 10.1080/23738871.2018.1513053, 10.3390/jmse11051021, and 10.1007/978-3-319-68496-3_15.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 3: Similarity Search\n",
    "query_text = \"Show me sources on innovative digitalization methods implemented in the maritime industry\"\n",
    "#query_text = \"Where can I find research about new navigational location techniques\"\n",
    "#query_text = \"Tell me where can I read about innovative ways insurance is being used in the boating industry\"\n",
    "\n",
    "query_embedding = modelRAG.encode([query_text])\n",
    "similarity_scores = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "\n",
    "# Get the indices of the top 3 most similar texts\n",
    "top_3_indices = np.argsort(similarity_scores)[-3:][::-1]\n",
    "\n",
    "RAG_string = \"\"\n",
    "for idx, index in enumerate(top_3_indices):\n",
    "    text = df.loc[index, 'answer']\n",
    "    respuesta_DOIs = df.loc[index, 'DOIs']\n",
    "    similarity_score = similarity_scores[index]\n",
    "    \n",
    "    # Use variables in a multi-line f-string\n",
    "    RAG_string += f\"\"\"\n",
    "Context {idx + 1}: {text}\n",
    "DOIs: {respuesta_DOIs}\n",
    "Similarity Score: {similarity_score:.4f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt\n",
    "prompt =\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "            You are a research assistant model.\n",
    "            Your objective is to retrieve Digital Object Identifiers (DOIs) of research papers that relate to the user's query.\n",
    "            Read the context provided and decide if it's relevant to what the user is asking. If it is, provide the DOIs for all the contexts that are relevant.\n",
    "            If you are unsure if the context is relevant for given question, try to not make stuff up and instead answer truthfully.\n",
    "            If there a DOIs, reply ONLY with \"A scientific article regarding those subjects can be found with the DOI\" (use the plural if needed), followed by the DOIs that are more relevant (if at all), don't explain your reasoning.\n",
    "            If there are no DOIs, DON'T MAKE THEM UP, explain you couldn't find the answer in your data sources.\n",
    "            If all similarity scores are below 0.45, tell the user you couldn't find research on that topic.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "            {}\n",
    "            Question: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# Format the prompt with RAG_string\n",
    "formatted_prompt = prompt.format(RAG_string, query_text)\n",
    "\n",
    "# Print the formatted prompt for debugging\n",
    "#print(formatted_prompt)\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate text\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f77624-aeab-4087-a88b-8eb7db982aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
