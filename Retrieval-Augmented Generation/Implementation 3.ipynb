{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e3a5a-3895-453d-9d04-98d97c168a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementaci√≥n 3 de vector embeddings - B√∫squeda de palabras clave extraidas con LLM\n",
    "## Se extrae del user_query palabras clave, que luego se procesan calculando la similitud de vectores\n",
    "## Con los vectores obtenidos se obtienen los contextos mas relevantes y sus DOIs y se introduen en el\n",
    "## system prompt del modelo con instrucciones de devolver S√ìLO los DOIs relevantes para el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f655f54-bd0c-4d59-90cd-63b830ce5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# Prefiero no ver el tiempo a ver los warnings (eliminamos %%time)\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Cargar y procesar el dataset (columna \"Context\", que contiene en este caso las oraciones extra√≠das de PDFs)\n",
    "df = pd.read_csv('cleaned_dataset2.csv')\n",
    "texts = df['Context'].tolist()\n",
    "\n",
    "# Generar Embeddings con el modelo all-MiniLM-L6-v2\n",
    "modelRAG = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = modelRAG.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c731fb4f-15ad-47d3-a4e4-04a6289ab3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: Quadro RTX 6000. Max memory: 23.645 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "CPU times: user 7.65 s, sys: 2.14 s, total: 9.79 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8694266e-7c98-4f70-b839-6ae16e73cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  7 19:51:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 6000                Off | 00000000:00:10.0 Off |                  Off |\n",
      "| 46%   60C    P8               9W / 260W |   6401MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     57160      C   .../d2tb/home/jugodu/envTFG/bin/python     6398MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ef864d-9eb0-4b98-b2f5-ca7287802795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n            You are a research assistant model.\\n            Your objective is to extract the main subjects that relate to the user's query.\\n            You should write a few words for each subject present in the query.\\n            For example:\\n            query: What are the key studies on the impact of high task-oriented interdependence in global supply chains and its significance for platform ecosystem adoption?\\n            assistant: high task-oriented interdependence, global supply chains, platform ecosystem adoption\\n            example 2:\\n            query: What are some authoritative sources on the process of encoding sequences of integers using the most frequent pairs?\\n            assistant: encoding sequences of integers, most frequent pairs\\n            <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n            Tell me where can I read about innovative ways insurance is being used in the boating industry<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ninnovative insurance, boating industry, maritime finance<|eot_id|>\"]\n",
      "innovative insurance, boating industry, maritime finance\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 3: Similarity Search\n",
    "#query_text = \"Where can I find research on positioning systems using physics models?\"\n",
    "#query_text = \"Where can I find research about new navigational location techniques\"\n",
    "query_text = \"Tell me where can I read about innovative ways insurance is being used in the boating industry\"\n",
    "\n",
    "\n",
    "# Define the prompt\n",
    "prompt =\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "            You are a research assistant model.\n",
    "            Your objective is to extract the main subjects that relate to the user's query.\n",
    "            You should write a few words for each subject present in the query.\n",
    "            For example:\n",
    "            query: What are the key studies on the impact of high task-oriented interdependence in global supply chains and its significance for platform ecosystem adoption?\n",
    "            assistant: high task-oriented interdependence, global supply chains, platform ecosystem adoption\n",
    "            example 2:\n",
    "            query: What are some authoritative sources on the process of encoding sequences of integers using the most frequent pairs?\n",
    "            assistant: encoding sequences of integers, most frequent pairs\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "            {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# Format the prompt with RAG_string\n",
    "formatted_prompt = prompt.format(query_text)\n",
    "\n",
    "# Print the formatted prompt for debugging\n",
    "#print(formatted_prompt)\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "## Inferencia para buscar dos preguntas de similar contexto \n",
    "## Pregunta extra 1\n",
    "model_answer1= model.generate(**inputs, temperature=0.7, max_new_tokens = 128)\n",
    "output1=tokenizer.batch_decode(model_answer1)\n",
    "\n",
    "## Funci√≥n para extraer SOLO el output del modelo.\n",
    "def output_cleaner(output):\n",
    "    # Convert list to string\n",
    "    output_str = output[0]\n",
    "    # Find the index of \"user\\n\\n\"\n",
    "    user_index = output_str.find(\"assistant<|end_header_id|>\\n\\n\")\n",
    "    # Slice the string to remove everything up to \"user\\n\\n\"\n",
    "    processed_output = output_str[user_index + len(\"assistant<|end_header_id|>\\n\\n\"):]\n",
    "    # Remove the last character eot\n",
    "    if processed_output.endswith(\"<|eot_id|>\"):\n",
    "        processed_output = processed_output[:-10]\n",
    "    return processed_output\n",
    "    \n",
    "embedding_search_query = output_cleaner(output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38861510-1409-436d-8c17-713ab1d0fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = modelRAG.encode([embedding_search_query])\n",
    "similarity_scores = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "\n",
    "# Get the indices of the top 3 most similar texts\n",
    "top_3_indices = np.argsort(similarity_scores)[-3:][::-1]\n",
    "\n",
    "RAG_string = \"\"\n",
    "for idx, index in enumerate(top_3_indices):\n",
    "    text = df.loc[index, 'Context']\n",
    "    respuesta_DOIs = df.loc[index, 'DOIs']\n",
    "    similarity_score = similarity_scores[index]\n",
    "    \n",
    "    # Use variables in a multi-line f-string\n",
    "    RAG_string += f\"\"\"\n",
    "Context {idx + 1}: {text}\n",
    "DOIs: {respuesta_DOIs}\n",
    "\n",
    "\"\"\"\n",
    "# Elimine Similarity Score: {similarity_score:.4f} para no sesgar al LLM\n",
    "print(RAG_string)\n",
    "\n",
    "# Define the prompt\n",
    "prompt =\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "            You are a helpful assistant.\n",
    "            Your objective is to retrieve Digital Object Identifiers (DOIs) of research papers that relate to the user's query.\n",
    "            Read the context provided and decide if it's relevant to what the user is asking. If it is, provide the DOIs for all the contexts that are relevant.\n",
    "            If you are unsure if the context is relevant for given question, try to not make stuff up and instead answer truthfully.\n",
    "            If there a DOIs, reply ONLY with \"A scientific article regarding those subjects can be found with the DOI\" (use the plural if needed), followed by the DOIs that are more relevant (if at all), don't explain your reasoning.\n",
    "            If there are no DOIs, DON'T MAKE THEM UP, explain you couldn't find the answer in your data sources.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "            {}\n",
    "            \n",
    "            User query: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# Elimine If all similarity scores are below 0.45, tell the user you couldn't find research on that topic. ya que quite los scores del RAG_string\n",
    "\n",
    "# Format the prompt with RAG_string\n",
    "formatted_prompt = prompt.format(RAG_string, query_text)\n",
    "\n",
    "# Print the formatted prompt for debugging\n",
    "#print(formatted_prompt)\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate text\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
